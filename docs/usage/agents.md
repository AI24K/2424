# Custom Assistant Guide

## Adding Custom Assistants

As the basic functional unit of LobeChat, adding and iterating assistants is crucial. Now you can add assistants to your favorites list in two ways.

### `A` Adding through the Role Market

If you are a novice at Prompt writing, you might want to browse the Assistant Market in LobeChat first. Here, you can find commonly used assistants submitted by others and easily add them to your list with just one click, which is very convenient.

![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279588466-4c32041b-a8e6-4703-ba4a-f91b7800e359.png)

### `B` Creating a New Custom Assistant

When you need to handle specific tasks, you should consider creating a custom assistant to help you solve the problem. You can add and configure the assistant in detail through the following steps.

![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279587283-a3ea8dfd-70fb-47ee-ab00-e3911ac6a939.png)
![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279587292-a3d102c6-f61e-4578-91f1-c0a4c97588e1.png)

> \[!NOTE]
>
> Quick Setup Tip: You can conveniently modify the Prompt through the shortcut edit button in the sidebar.

![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279587294-388d1877-193e-4a50-9fe8-8fbcc3ccefa0.png)
![](https://github-production-user-asset-6210df.s3.amazonaws.com/17870709/279587298-333da153-13b8-4557-a0a2-cff55e7bc1c0.png)

Please continue reading to understand Prompt writing techniques and common model parameter settings.

<br/>

## Prompt Basic Concepts

Generative AI is very useful, but it requires human guidance. In general, generative AI can be as capable as a new intern at a company, but it needs clear instructions to perform well. Being able to guide generative AI correctly is a very powerful skill. You can guide generative AI by sending a prompt, which is usually a text command. The prompt is the input provided to the assistant, and it will affect the output. A good prompt should be structured, clear, concise, and directive.

### How to Write a Structured Prompt

> \[!TIP]
>
> A structured prompt refers to the construction of the prompt having a clear logic and structure. For example, if you want the model to generate an article, your prompt may need to include information such as the topic of the article, the outline of the article, and the style of the article.

Let's look at a basic example of a discussion prompt:

> _"What are the most urgent environmental issues facing our planet, and what can individuals do to help address these issues?"_

We can convert this into a simple prompt for the assistant to answer the question:

```
Answer the following question:
What are the most urgent environmental issues facing our planet, and what can individuals do to help address these issues?
```

Since the results generated by this prompt are not consistent, some are only one or two sentences. A typical discussion response should have multiple paragraphs, so these results are not ideal. A good prompt should provide **specific formatting and content instructions**. You need to eliminate ambiguity in the language to improve consistency and quality. Here is a better prompt:

```
Write a highly detailed paper, including an introduction, body paragraphs, and a conclusion, to answer the following question:
What are the most urgent environmental issues facing our planet,
and what can individuals do to help address these issues?
```

The second prompt generates longer output and a better structure. The use of the term "paper" in the prompt is intentional, as the assistant can understand the definition of a paper, making it more likely to generate coherent, structured responses.

<br/>

### How to Improve Quality and Effectiveness

> \[!TIP]
>
> There are several ways to improve the quality and effectiveness of prompts:
>
> - Clearly define your requirements. The model's output will strive to meet your needs, so if your requirements are unclear, the output may not meet expectations.
> - Use correct grammar and spelling. The model will attempt to mimic your language style, so if your language style is flawed, the output may also be problematic.
> - Provide sufficient contextual information. The model will generate output based on the context you provide, so if the contextual information is insufficient, it may not produce the desired results.

After formulating effective prompts for discussing issues, you now need to refine the generated results. This may involve adjusting the output to fit constraints such as word count, or combining concepts from different generated results.

A simple method of iteration is to generate multiple outputs and review them to understand the concepts and structure being used. Once the outputs have been evaluated, you can select the most suitable ones and combine them into a coherent response. Another iterative method is to start small and then **gradually expand**. This requires more than one prompt: an initial prompt for drafting the initial one or two paragraphs, followed by additional prompts to expand on the content already written. Here is a potential prompt for a philosophical discussion question:

> _"Is mathematics an invention or a discovery? Use careful reasoning to explain your answer."_

Add it to a simple prompt as follows:

```
Write an opening paragraph to answer the following question:
Is mathematics an invention or a discovery? Use careful reasoning to explain your answer.
```

I generated some responses and found one that I liked:

```
Philosophers and mathematicians have long debated whether mathematics is an invention or a discovery.
Some argue that the concepts and principles of mathematics exist independently of human thought, and therefore can be considered discovered rather than invented.
For example, the concepts of numbers, ratios, and geometric shapes are observed in the natural world and can be abstracted and systematically studied.
```

It's a good start. Then I used another prompt to expand:

````
I am writing a detailed essay to answer the following prompt:
Is mathematics an invention or a discovery? Use careful reasoning to explain your answer. Here is my current situation: Philosophers and mathematicians have long debated whether mathematics is an invention or a discovery.
Some argue that the concepts and principles of mathematics exist independently of human thought, and therefore can be considered discovered rather than invented.
For example, the concepts of numbers, ratios, and geometric shapes are observed in the natural world and can be abstracted and systematically studied. Write the next paragraph of my essay.

```markdown
I used this prompt to generate several extensions and selected one that I liked, resulting in the following:

---

The debate between philosophers and mathematicians continues: is mathematics invented or discovered? Some argue that the concepts and principles of mathematics exist independently of human thought, and therefore can be considered a discovery rather than an invention. For example, the concepts of numbers, ratios, and geometric shapes are observed in the natural world and can be abstracted and systematized through study. On the other hand, some believe that mathematics is largely an invention of human thought. Mathematical principles are based on logical reasoning, a characteristic of human cognition. For instance, Euclidean geometry, based on abstract axioms and premises, is accepted as true without the need for proof. Therefore, geometry can be seen as an invention of human thought rather than a discovery. Similarly, mathematical formulas and equations are used to model and predict physical phenomena, which are the result of human reasoning.

Using the prompt extensions, we can gradually write and iterate at each step. This is particularly useful for cases that require generating higher quality output and incremental modifications.
````

## Concept of Models

### ChatGPT

- **gpt-3.5-turbo**: Currently the fastest generating ChatGPT model, it is faster but may sacrifice some text generation quality, with a context length of 4k.
- **gpt-3.5-turbo-16k**: Similar to gpt-4, the context limit is increased to 16k tokens, with a higher cost.
- **gpt-4**: ChatGPT 4.0 has improved language understanding and generation capabilities compared to 3.5. It can better understand context and context and generate more accurate and natural responses. This is due to improvements in the GPT-4 model, including better language modeling and deeper semantic understanding, but it may be slower than other models, with a context length of 8k.
- **gpt-4-32k**: Similar to gpt-4, the context limit is increased to 32k tokens, with a higher cost.

<br/>

## Concept of Model Parameters

LLM seems magical, but it is essentially a probability problem. The neural network generates a bunch of candidate words from the pre-trained model based on the input text and selects the high-probability ones as output. Most of the related parameters are associated with sampling (i.e., how to select the output from the candidate words).

### `temperature`

This parameter controls the randomness of the model's output. The larger the value, the greater the randomness. Generally, when the same prompt is input multiple times, the model's output varies each time.

- Set to 0: Generates a fixed output for each prompt
- Lower values: More concentrated and deterministic output
- Higher values: More random output (more creative)

> \[!NOTE]
>
> Generally, the longer and clearer the prompt, the better the quality and confidence of the model's generated output. In this case, the temperature value can be adjusted appropriately. Conversely, if the prompt is short and ambiguous, setting a relatively high temperature value will result in unstable model output.

<br/>

### `top_p`

Top-p nucleus sampling is also a sampling parameter, different from temperature. Before outputting, the model generates a bunch of tokens, ranks them based on quality, and dynamically selects candidate words from the tokens based on a percentage in the nucleus sampling mode. Top-p introduces randomness in selecting tokens, giving other high-scoring tokens a chance to be chosen instead of always selecting the highest-scoring one.

> \[!NOTE]
>
> Top-p is similar to randomness, and it is generally not recommended to change it together with the randomness parameter, temperature.

<br/>

### `presence_penalty`

The presence penalty parameter can be seen as a punishment for repetitive content in the generated text. When set to a higher value, the generation model will try to avoid producing repeated words, phrases, or sentences. Conversely, if the presence penalty parameter is set to a lower value, the generated text may contain more repetitive content. By adjusting the value of the presence penalty parameter, control over the originality and diversity of the generated text can be achieved. The importance of this parameter is mainly reflected in the following aspects:

- Enhancing the originality and diversity of the generated text: In some applications, such as creative writing and generating news headlines, the generated text needs to have high originality and diversity. By increasing the value of the presence penalty parameter, the amount of repetitive content in the generated text can be effectively reduced, thereby enhancing its originality and diversity.
- Preventing the generation of loops and meaningless content: In some cases, the generation model may produce repetitive or meaningless text that usually fails to convey useful information. By appropriately increasing the value of the presence penalty parameter, the probability of generating such meaningless content can be reduced, thereby improving the readability and practicality of the generated text.

> \[!NOTE]
>
> It is worth noting that the presence penalty parameter, in conjunction with other parameters such as temperature and top-p, collectively influences the quality of the generated text. Compared to other parameters, the presence penalty parameter mainly focuses on the originality and repetitiveness of the text, while the temperature and top-p parameters more significantly affect the randomness and determinism of the generated text. By adjusting these parameters reasonably, comprehensive control over the quality of the generated text can be achieved.

<br/>

### `frequency_penalty`

It is a mechanism that imposes penalties on newly frequent words in the text to reduce the likelihood of the model repeating the same words. The higher the value, the more likely it is to reduce the repetition of words.

- `-2.0` When the morning news started, I found my TV now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now

## Further Reading

- **Learn Prompting** - [Learn Prompting](https://learnprompting.org/en-US/docs/intro)
