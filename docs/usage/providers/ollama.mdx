---
title: Using Ollama in LobeChat
description: 了解如何在 LobeChat 中使用 Ollama 框架，以增强您的应用程序。
tags:
  - Ollama
  - LobeChat
  - 本地大语言模型
  - Ollama WebUI
---
# Using Ollama in LobeChat

![Using Ollama in LobeChat](https://github.com/lobehub/lobe-chat/assets/28616219/a2a091b8-ac45-4679-b5e0-21d711e17fef)

Ollama is a powerful framework for running large language models (LLMs) locally, supporting various language models such as Llama 2, Mistral, and more. Now, LobeChat supports integration with Ollama, meaning you can easily enhance your application by using the language models provided by Ollama in LobeChat.

This document will guide you on how to use Ollama in LobeChat:

## Using Ollama on macOS

<Steps>

### Installing Ollama locally

```
[Download Ollama for macOS](https://ollama.com/download?utm_source=lobehub&utm_medium=docs&utm_campaign=download-macos) and unzip, then install it.
```

### Configuring Ollama for cross-origin access

Due to Ollama's default configuration, it is set to only allow local access at startup. Additional environment variable setting `OLLAMA_ORIGINS` is required for cross-origin access and port listening. Use `launchctl` to set the environment variable:

```bash
launchctl setenv OLLAMA_ORIGINS "*"
```

After completing the setup, you need to restart the Ollama application.

### Conversing with the local large model in LobeChat

Now, you can start conversing with the local LLM in LobeChat.

</Steps>

## Using Ollama on Windows

<Steps>

### Installing Ollama locally

```
[Download Ollama for Windows](https://ollama.com/download?utm_source=lobehub&utm_medium=docs&utm_campaign=download-windows) and install it.
```

### Configuring Ollama for cross-origin access

Due to Ollama's default configuration, it is set to only allow local access at startup. Additional environment variable setting `OLLAMA_ORIGINS` is required for cross-origin access and port listening.

On Windows, Ollama inherits your user and system environment variables.

1. First, exit the Ollama program by clicking on it in the Windows taskbar.
2. Edit system environment variables from the control panel.
3. Edit or create the environment variable `OLLAMA_ORIGINS` for your user account, set the value to `*`.
4. Click `OK/Apply` to save and restart the system.
5. Run `Ollama` again.

### Conversing with the local large model in LobeChat

Now, you can start conversing with the local LLM in LobeChat.

</Steps>

## Using Ollama on Linux

<Steps>

### Installing Ollama locally

Install using the following command:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Alternatively, you can refer to the [Linux manual installation guide](https://github.com/ollama/ollama/blob/main/docs/linux.md).

### Configuring Ollama for cross-origin access

Due to Ollama's default configuration, it is set to only allow local access at startup. Additional environment variable setting `OLLAMA_ORIGINS` is required for cross-origin access. If Ollama runs as a systemd service, use `systemctl` to set the environment variable:

1. Edit the systemd service by calling `sudo systemctl edit ollama.service`:

```bash
sudo systemctl edit ollama.service
```

2. Add `Environment` for each environment variable under `[Service]`:

```bash
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=*"
```

3. Save and exit.
4. Reload `systemd` and restart Ollama:

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### Conversing with the local large model in LobeChat

Now, you can start conversing with the local LLM in LobeChat.

</Steps>

## Deploying Ollama using Docker

<Steps>

### Pulling the Ollama image

If you prefer using Docker, Ollama also provides an official Docker image. You can pull it using the following command:

```bash
docker pull ollama/ollama
```

### Configuring Ollama for cross-origin access

Due to Ollama's default configuration, it is set to only allow local access at startup. Additional environment variable setting `OLLAMA_ORIGINS` is required for cross-origin access.

If Ollama runs as a Docker container, you can add the environment variable to the `docker run` command.

```bash
docker run -d --gpus=all -v ollama:/root/.ollama -e OLLAMA_ORIGINS="*" -p 11434:11434 --name ollama ollama/ollama
```

### Conversing with the local large model in LobeChat

Now, you can start conversing with the local LLM in LobeChat.

</Steps>

## Installing Ollama Models

Ollama supports various models, and you can check the available model list in the [Ollama Library](https://ollama.com/library) and choose the appropriate model based on your needs.

### Installing in LobeChat

In LobeChat, we have enabled some common large language models by default, such as llama3, Gemma, Mistral, etc. When you select a model for conversation, we will prompt you to download the model.

![LobeChat prompts to install Ollama model](https://github.com/lobehub/lobe-chat/assets/28616219/4e81decc-776c-43b8-9a54-dfb43e9f601a)

Once downloaded, you can start the conversation.

### Pulling models to local using Ollama

Alternatively, you can install models by executing the following command in the terminal, using llama3 as an example:

```bash
ollama pull llama3
```

![Video](https://github.com/lobehub/lobe-chat/assets/28616219/95828c11-0ae5-4dfa-84ed-854124e927a6)

<Callout type={'info'}>
You can visit [Integrating with Ollama](/docs/self-hosting/examples/ollama) to learn how to deploy LobeChat to meet integration needs with Ollama.
</Callout>

## Custom Configuration

You can find Ollama's configuration options in `Settings` -> `Language Model`, where you can configure Ollama's proxy, model names, etc.

![Ollama Service Provider Settings](https://github.com/lobehub/lobe-chat/assets/28616219/54b3696b-5b13-4761-8c1b-1e664867b2dd)

